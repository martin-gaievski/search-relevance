name: Tests with LLM Support

on:
  push:
    branches: [ main, feature/* ]
  pull_request:
    branches: [ main ]

jobs:
  # Third job using Ollama with a lightweight model
  test-with-ollama:
    runs-on: ubuntu-latest
    # if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'

    - name: Setup Ollama
      run: |
        # Download and install Ollama
        echo "Installing Ollama..."
        curl -fsSL https://ollama.com/install.sh | sh

        # Start Ollama service in background
        echo "Starting Ollama service..."
        ollama serve &
        OLLAMA_PID=$!
        echo "OLLAMA_PID=$OLLAMA_PID" >> $GITHUB_ENV

        # Wait for Ollama to be ready
        echo "Waiting for Ollama to start..."
        for i in {1..30}; do
          if curl -s http://localhost:11434/api/version > /dev/null; then
            echo "Ollama service started successfully"
            break
          fi
          echo "Attempt $i/30: Still waiting..."
          sleep 2
        done

        # Pull the smallest available model for CI (tinyllama is fastest)
        echo "Pulling TinyLlama model (optimized for CI)..."
        ollama pull tinyllama

        # Verify model is available
        echo "Available models:"
        ollama list

    - name: Test Ollama API
      run: |
        # Test Ollama API compatibility
        echo "Testing Ollama API..."

        # Test models endpoint
        curl -s http://localhost:11434/api/tags | jq .

        # Test chat completion
        curl -s -X POST http://localhost:11434/api/chat \
          -H "Content-Type: application/json" \
          -d '{
            "model": "tinyllama",
            "messages": [{"role": "user", "content": "Hello! Are you working?"}],
            "stream": false
          }' | jq .

    - name: Create Ollama Bridge
      run: |
        # Create a bridge server to make Ollama compatible with OpenAI API format
        cat > ollama_bridge.py <<'EOF'
        from http.server import HTTPServer, BaseHTTPRequestHandler
        import json
        import requests
        import uuid
        import traceback
        from datetime import datetime

        class OllamaHandler(BaseHTTPRequestHandler):
            def do_GET(self):
                try:
                    if self.path == '/v1/models':
                        self.send_response(200)
                        self.send_header('Content-Type', 'application/json')
                        self.end_headers()
                        response = {
                            "data": [
                                {
                                    "id": "tinyllama",
                                    "object": "model",
                                    "created": int(datetime.now().timestamp()),
                                    "owned_by": "ollama"
                                }
                            ]
                        }
                        self.wfile.write(json.dumps(response).encode())
                    else:
                        self.send_response(404)
                        self.end_headers()
                except Exception as e:
                    print(f"GET Error: {e}")
                    traceback.print_exc()
                    self.send_response(500)
                    self.end_headers()

            def do_POST(self):
                try:
                    content_length = int(self.headers.get('Content-Length', 0))
                    if content_length == 0:
                        self.send_response(400)
                        self.end_headers()
                        return

                    post_data = self.rfile.read(content_length)
                    request_data = json.loads(post_data.decode('utf-8'))

                    print(f"Received request: {json.dumps(request_data, indent=2)}")

                    if self.path == '/v1/chat/completions':
                        # Convert OpenAI format to Ollama format
                        ollama_request = {
                            "model": "tinyllama",
                            "messages": request_data.get("messages", []),
                            "stream": False
                        }

                        print(f"Sending to Ollama: {json.dumps(ollama_request, indent=2)}")

                        # Call Ollama API with longer timeout
                        ollama_response = requests.post(
                            "http://localhost:11434/api/chat",
                            json=ollama_request,
                            timeout=60
                        )

                        print(f"Ollama status: {ollama_response.status_code}")
                        print(f"Ollama response: {ollama_response.text}")

                        if ollama_response.status_code == 200:
                            ollama_data = ollama_response.json()
                            content = ollama_data.get("message", {}).get("content", "")

                            # Convert Ollama response to OpenAI format
                            openai_response = {
                                "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                                "object": "chat.completion",
                                "created": int(datetime.now().timestamp()),
                                "model": "tinyllama",
                                "choices": [
                                    {
                                        "index": 0,
                                        "message": {
                                            "role": "assistant",
                                            "content": content
                                        },
                                        "finish_reason": "stop"
                                    }
                                ],
                                "usage": {
                                    "prompt_tokens": 100,
                                    "completion_tokens": len(content.split()),
                                    "total_tokens": 100 + len(content.split())
                                }
                            }

                            print(f"Sending OpenAI response: {json.dumps(openai_response, indent=2)}")

                            self.send_response(200)
                            self.send_header('Content-Type', 'application/json')
                            self.end_headers()
                            self.wfile.write(json.dumps(openai_response).encode())
                        else:
                            print(f"Ollama error: {ollama_response.status_code} - {ollama_response.text}")
                            self.send_response(502)
                            self.send_header('Content-Type', 'application/json')
                            self.end_headers()
                            error_response = {"error": {"message": f"Ollama returned {ollama_response.status_code}", "type": "api_error"}}
                            self.wfile.write(json.dumps(error_response).encode())
                    else:
                        self.send_response(404)
                        self.end_headers()

                except Exception as e:
                    print(f"POST Error: {e}")
                    traceback.print_exc()
                    self.send_response(500)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    error_response = {"error": {"message": str(e), "type": "internal_error"}}
                    self.wfile.write(json.dumps(error_response).encode())

            def log_message(self, format, *args):
                print(f"[{datetime.now()}] {format % args}")

        if __name__ == '__main__':
            server = HTTPServer(('localhost', 8080), OllamaHandler)
            print("Ollama bridge server started on http://localhost:8080")
            server.serve_forever()
        EOF

        # Install required Python package
        pip3 install requests

        # Start the bridge server with logs
        echo "Starting Ollama bridge..."
        python3 ollama_bridge.py > bridge.log 2>&1 &
        BRIDGE_PID=$!
        echo "BRIDGE_PID=$BRIDGE_PID" >> $GITHUB_ENV

        # Wait for bridge to start
        sleep 5

        # Test the bridge
        echo "Testing Ollama bridge..."
        curl -v http://localhost:8080/v1/models | jq . || echo "Bridge test failed"

        # Show bridge logs
        echo "Bridge logs:"
        tail -n 20 bridge.log || echo "No bridge logs yet"

    - name: Run integration tests with Ollama
      env:
        LOCALAI_API_URL: http://localhost:8080
        LLM_MODEL_NAME: tinyllama
      run: |
        ./gradlew integTest --tests "*LLMJudgmentGenerationIT" -Dtests.cluster.llm.enabled=true

    - name: Stop Ollama and Bridge
      if: always()
      run: |
        if [ ! -z "$BRIDGE_PID" ]; then
          echo "Stopping Ollama bridge..."
          kill $BRIDGE_PID || true
        fi
        if [ ! -z "$OLLAMA_PID" ]; then
          echo "Stopping Ollama service..."
          kill $OLLAMA_PID || true
        fi
