name: Tests with LLM Support

on:
  push:
    branches: [ main, feature/* ]
  pull_request:
    branches: [ main ]

jobs:
  test-with-mock-llm:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'

    - name: Setup Gradle
      uses: gradle/gradle-build-action@v2

    - name: Create Mock LLM Server
      run: |
        # Create a simple mock LLM server using Python
        cat > mock_llm_server.py <<'EOF'
        from http.server import HTTPServer, BaseHTTPRequestHandler
        import json
        import uuid
        from datetime import datetime

        class MockLLMHandler(BaseHTTPRequestHandler):
            def do_GET(self):
                if self.path == '/v1/models':
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()
                    response = {
                        "data": [
                            {
                                "id": "mock-model",
                                "object": "model",
                                "created": int(datetime.now().timestamp()),
                                "owned_by": "mock"
                            }
                        ]
                    }
                    self.wfile.write(json.dumps(response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def do_POST(self):
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)

                if self.path == '/v1/chat/completions':
                    self.send_response(200)
                    self.send_header('Content-Type', 'application/json')
                    self.end_headers()

                    # Mock response for relevance evaluation
                    response = {
                        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
                        "object": "chat.completion",
                        "created": int(datetime.now().timestamp()),
                        "model": "mock-model",
                        "choices": [
                            {
                                "index": 0,
                                "message": {
                                    "role": "assistant",
                                    "content": "Based on the query and document, I would rate the relevance as 0.85 out of 1.0. The document directly addresses the query about smartphones with good cameras."
                                },
                                "finish_reason": "stop"
                            }
                        ],
                        "usage": {
                            "prompt_tokens": 10,
                            "completion_tokens": 20,
                            "total_tokens": 30
                        }
                    }
                    self.wfile.write(json.dumps(response).encode())
                else:
                    self.send_response(404)
                    self.end_headers()

            def log_message(self, format, *args):
                # Suppress log messages
                pass

        if __name__ == '__main__':
            server = HTTPServer(('localhost', 8080), MockLLMHandler)
            print("Mock LLM server started on http://localhost:8080")
            server.serve_forever()
        EOF

        # Start the mock server in the background
        python3 mock_llm_server.py &
        MOCK_SERVER_PID=$!
        echo "MOCK_SERVER_PID=$MOCK_SERVER_PID" >> $GITHUB_ENV

        # Wait for server to start
        sleep 2

        # Test the mock server
        curl -s http://localhost:8080/v1/models | jq .

    - name: Run integration tests with Mock LLM
      env:
        LOCALAI_API_URL: http://localhost:8080
        LLM_MODEL_NAME: mock-model
      run: |
        ./gradlew integTest --tests "*LLMJudgmentGenerationIT" -Dtests.cluster.llm.enabled=true

    - name: Stop Mock Server
      if: always()
      run: |
        if [ ! -z "$MOCK_SERVER_PID" ]; then
          kill $MOCK_SERVER_PID || true
        fi

  # Alternative job using llamafile (single binary, ~4GB but faster than LocalAI)
  test-with-llamafile:
    runs-on: ubuntu-latest
    # if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'

    - name: Setup Llamafile
      run: |
        # Download TinyLlama llamafile (smallest available, ~600MB)
        wget -q https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.llamafile
        chmod +x TinyLlama-1.1B-Chat-v1.0.Q4_K_M.llamafile

        # Start llamafile server
        ./TinyLlama-1.1B-Chat-v1.0.Q4_K_M.llamafile --server --port 8080 --nobrowser &
        LLAMAFILE_PID=$!
        echo "LLAMAFILE_PID=$LLAMAFILE_PID" >> $GITHUB_ENV

        # Wait for server to start (may take a minute)
        echo "Waiting for llamafile to start..."
        for i in {1..60}; do
          if curl -s http://localhost:8080/v1/models > /dev/null; then
            echo "Llamafile started successfully"
            break
          fi
          sleep 2
        done

    - name: Run integration tests with Llamafile
      env:
        LOCALAI_API_URL: http://localhost:8080
        LLM_MODEL_NAME: TinyLlama-1.1B-Chat-v1.0
      run: |
        ./gradlew integTest --tests "*LLMJudgmentGenerationIT" -Dtests.cluster.llm.enabled=true

    - name: Stop Llamafile
      if: always()
      run: |
        if [ ! -z "$LLAMAFILE_PID" ]; then
          kill $LLAMAFILE_PID || true
        fi
